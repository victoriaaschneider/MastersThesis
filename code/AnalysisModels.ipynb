{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51f0f3-6269-44c6-9955-e0df455fec45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SET UP AND DATA LOAD\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df = pd.read_csv('df_finalized.csv')\n",
    "\n",
    "# Make sure column names are clean\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Convert mostImportantDateUTC to datetime\n",
    "df['mostImportantDateUTC'] = pd.to_datetime(df['mostImportantDateUTC'])\n",
    "\n",
    "# Load the GVKEY to ticker crosswalk\n",
    "gvkey_ticker = pd.read_csv('merged_wrds_gvkey_V2.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a46957-73f9-414b-9b5f-b09c20945e7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## CALCULATE FINANCIAL METRIC CHANGES BETWEEN QUARTERS\n",
    "\n",
    "df_change = df.sort_values(by=['gvkey', 'mostImportantDateUTC'])\n",
    "metrics = ['eps', 'pe_ratio', 'ev_ebitda', 'revenue']\n",
    "\n",
    "for metric in metrics:\n",
    "    df_change[f'{metric}_change'] = df.groupby('gvkey')[metric].pct_change()\n",
    "\n",
    "print(df_change.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a07652-d1c3-4148-9d6a-04aa1e90c6d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## FIXED EFFECTS TEST\n",
    "\n",
    "\n",
    "# Columns needed for regression\n",
    "fundamentals = ['eps_change', 'pe_ratio_change', 'ev_ebitda_change', 'revenue_change']\n",
    "sentiment_vars = ['total_sentiment_score', 'average_sentiment_score', 'sentiment_ratio', 'volatility']\n",
    "\n",
    "# Only drop rows with NaNs in variables you will use\n",
    "all_vars = sentiment_vars + fundamentals + ['Price_Pct_Change']\n",
    "df_clean = df_change.dropna(subset=all_vars)\n",
    "\n",
    "# Make sure index columns exist\n",
    "if 'gvkey' in df_clean.columns and 'mostImportantDateUTC' in df_clean.columns:\n",
    "    df_clean = df_clean.set_index(['gvkey', 'mostImportantDateUTC'])\n",
    "\n",
    "\n",
    "# Loop through each fundamental variable\n",
    "for dep_var in fundamentals:\n",
    "    print(f\"=== Fixed Effects Panel Regression for {dep_var} ===\")\n",
    "    \n",
    "    # Build formula\n",
    "    formula = f\"{dep_var} ~ {' + '.join(indep_vars)} + EntityEffects + TimeEffects\"\n",
    "    \n",
    "    # Fit model\n",
    "    model = PanelOLS.from_formula(formula, data=df_change_clean)\n",
    "    results = model.fit(cov_type='clustered', cluster_entity=True)\n",
    "    \n",
    "    # Print summary\n",
    "    print(results.summary)\n",
    "    print(\"\\n\" + \"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f299d2-244a-491d-802d-2679e104b852",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## CORRELATIONS BETWEEN FINANCIALS AND SENTIMENT\n",
    "\n",
    "sentiment_cols = ['total_sentiment_score', 'average_sentiment_score', 'sentiment_ratio', 'volatility']\n",
    "financial_changes = ['Price_Pct_Change', 'eps_change', 'pe_ratio_change', 'ev_ebitda_change', 'revenue_change' ]\n",
    "df_clean = df.dropna(subset=financial_changes)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_clean[sentiment_cols + financial_changes].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation between Sentiment and Financial Metrics\")\n",
    "plt.show()\n",
    "\n",
    "for col in sentiment_cols:\n",
    "    for target in financial_changes:\n",
    "        sns.lmplot(x=col, y=target, data=df, height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e237a-9d48-4c5a-8ed4-51f55997a780",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## LOGISITIC REGRESSION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- 1. Define outcome variables ---\n",
    "df['EPS_up'] = (df['eps_change'] > 0).astype(int)\n",
    "df['Revenue_up'] = (df['revenue_change'] > 0).astype(int)\n",
    "df['PE_up'] = (df['pe_ratio_change'] > 0).astype(int)\n",
    "df['EV_EBITDA_up'] = (df['ev_ebitda_change'] > 0).astype(int)\n",
    "df['Price_up'] = (df['Price_Pct_Change'] > 0).astype(int)\n",
    "\n",
    "# --- 2. Define features (avoid perfect collinearity) ---\n",
    "features = ['total_sentiment_score', 'average_sentiment_score', 'volatility']  \n",
    "# Remove 'sentiment_ratio' if it's derived from other features\n",
    "\n",
    "# --- 3. Prepare results storage ---\n",
    "results = {}\n",
    "\n",
    "# --- 4. Loop through each outcome ---\n",
    "for outcome in ['EPS_up', 'Revenue_up', 'PE_up', 'EV_EBITDA_up', 'Price_up']:\n",
    "    # Drop missing values for this outcome\n",
    "    df_clean = df.dropna(subset=features + [outcome])\n",
    "    \n",
    "    X = df_clean[features]\n",
    "    y = df_clean[outcome]\n",
    "    \n",
    "    # Add constant for intercept\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Fit logistic regression\n",
    "    logit_model = sm.Logit(y, X).fit(disp=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_prob = logit_model.predict(X)\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    roc_auc = roc_auc_score(y, y_pred_prob)\n",
    "    \n",
    "    # Coefficients and odds ratios\n",
    "    coeffs = logit_model.params\n",
    "    odds_ratios = np.exp(coeffs)\n",
    "    \n",
    "    # Store results\n",
    "    results[outcome] = {\n",
    "        'features': X.columns.tolist(),\n",
    "        'coeffs': coeffs.values,\n",
    "        'odds_ratios': odds_ratios.values,\n",
    "        'y_true': y.values,\n",
    "        'y_pred': y_pred,\n",
    "        'probs': y_pred_prob\n",
    "    }\n",
    "\n",
    "# Create a folder to save plots\n",
    "plot_dir = \"logistic_plots\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "def visualize_logistic_results(model_name, data):\n",
    "    features = data['features']\n",
    "    coeffs = data['coeffs']\n",
    "    odds_ratios = data['odds_ratios']\n",
    "    y_true = data['y_true']\n",
    "    y_pred = data['y_pred']\n",
    "    probs = data['probs']\n",
    "\n",
    "    # --- Feature coefficients ---\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.barplot(x=features, y=coeffs, palette='coolwarm')\n",
    "    plt.title(f\"{model_name} - Feature Coefficients\")\n",
    "    plt.ylabel(\"Coefficient Value\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, f\"{model_name}_coefficients.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Odds ratios ---\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.barplot(x=features, y=odds_ratios, palette='viridis')\n",
    "    plt.title(f\"{model_name} - Feature Odds Ratios\")\n",
    "    plt.ylabel(\"Odds Ratio\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, f\"{model_name}_odds_ratios.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Confusion Matrix ---\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, f\"{model_name}_confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- ROC Curve ---\n",
    "    fpr, tpr, _ = roc_curve(y_true, probs)\n",
    "    roc_auc = roc_auc_score(y_true, probs)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})', color='darkorange', lw=2)\n",
    "    plt.plot([0,1], [0,1], 'k--', lw=1)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"{model_name} - ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, f\"{model_name}_roc_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# --- Visualize and save all models ---\n",
    "for model_name, data in results.items():\n",
    "    visualize_logistic_results(model_name, data)\n",
    "\n",
    "print(f\"All plots saved to folder: {plot_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03b1c4-42f6-4fd8-979a-d8352fd3257b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## LOGISTIC REGRESSION\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Loop through each model and print a summary\n",
    "for model_name, data in results.items():\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    \n",
    "    # Convert features, coefficients, and odds ratios into a DataFrame\n",
    "    if 'odds_ratios' in data:\n",
    "        coeffs_df = pd.DataFrame({\n",
    "            'Feature': data['features'],\n",
    "            'Coefficient': data['coeffs'],\n",
    "            'Odds_Ratio': data['odds_ratios']\n",
    "        })\n",
    "    else:\n",
    "        coeffs_df = pd.DataFrame({\n",
    "            'Feature': data['features'],\n",
    "            'Coefficient': data['coeffs']\n",
    "        })\n",
    "    \n",
    "    print(\"\\nFeature Coefficients and Odds Ratios:\")\n",
    "    print(coeffs_df.to_string(index=False))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    if 'confusion_matrix' in data:\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(data['confusion_matrix'])\n",
    "    \n",
    "    # Classification metrics\n",
    "    y_true = data['y_true']\n",
    "    y_pred = data['y_pred']\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    \n",
    "    # ROC AUC\n",
    "    if 'probs' in data:\n",
    "        roc_auc = roc_auc_score(y_true, data['probs'])\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51694ba1-1225-4309-9fa5-a47f6640d6d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## EVENT STUDY - PULLING ALL STOCK DATA FOR EACH TICKER 2017-2022\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "\n",
    "# --- Load transcript data ---\n",
    "df_transcripts = pd.read_csv('df_finalized.csv')\n",
    "df_transcripts['mostImportantDateUTC'] = pd.to_datetime(df_transcripts['mostImportantDateUTC'])\n",
    "\n",
    "# --- Load GVKEY to ticker crosswalk ---\n",
    "gvkey_ticker = pd.read_csv('merged_wrds_gvkey_V2.txt', sep='\\t')\n",
    "df_transcripts = df_transcripts.merge(gvkey_ticker[['gvkey', 'tic']], on='gvkey', how='left')\n",
    "df_transcripts.rename(columns={'tic': 'ticker'}, inplace=True)\n",
    "df_transcripts['ticker'] = df_transcripts['ticker'].astype(str).str.upper().str.strip()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Directory to save stock data\n",
    "os.makedirs(\"stock_data\", exist_ok=True)\n",
    "\n",
    "def get_stock_data(ticker, start=\"2017-01-01\", end=\"2022-12-31\"):\n",
    "    \"\"\"\n",
    "    Downloads and saves stock data for a single ticker.\n",
    "    Returns True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    filename = f\"stock_data/{ticker}.csv\"\n",
    "    \n",
    "    # Skip if already downloaded\n",
    "    if os.path.exists(filename):\n",
    "        return f\"{ticker}: already exists\"\n",
    "\n",
    "    try:\n",
    "        stock = yf.download(ticker, start=start, end=end, progress=False)\n",
    "        if stock.empty:\n",
    "            return f\"{ticker}: no data\"\n",
    "        \n",
    "        stock.to_csv(filename)\n",
    "        return f\"{ticker}: downloaded\"\n",
    "    except Exception as e:\n",
    "        return f\"{ticker}: failed ({e})\"\n",
    "\n",
    "\n",
    "def download_all_tickers(ticker_list, start=\"2017-01-01\", end=\"2022-12-31\", max_workers=10):\n",
    "    \"\"\"\n",
    "    Downloads stock data for many tickers in parallel.\n",
    "    max_workers controls how many tickers are pulled simultaneously.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(get_stock_data, t, start, end): t for t in ticker_list}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "results = download_all_tickers(\n",
    "    tickers, \n",
    "    start=\"2017-01-01\", \n",
    "    end=\"2022-12-31\", \n",
    "    max_workers=8\n",
    ")\n",
    "# Save a log of results\n",
    "pd.DataFrame(results, columns=[\"status\"]).to_csv(\"download_log.csv\", index=False)\n",
    "\n",
    "print(\"Done. Check download_log.csv for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb4fe6-9916-4f90-b6d4-4938477fffa7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## EVENT STUDY - MATCH EVENT WINDOW STOCK PRICES TO TRANSCRIPTS, COMPUTE RETURNS, CALCULATE CAR\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Make sure folder exists\n",
    "os.makedirs(\"stock_data\", exist_ok=True)\n",
    "\n",
    "# --- Load transcript data ---\n",
    "df_transcripts = pd.read_csv('df_finalized.csv')\n",
    "df_transcripts['mostImportantDateUTC'] = pd.to_datetime(df_transcripts['mostImportantDateUTC'])\n",
    "\n",
    "# --- Load GVKEY to ticker crosswalk ---\n",
    "gvkey_ticker = pd.read_csv('merged_wrds_gvkey_V2.txt', sep='\\t')\n",
    "df_transcripts = df_transcripts.merge(gvkey_ticker[['gvkey', 'tic']], on='gvkey', how='left')\n",
    "df_transcripts.rename(columns={'tic': 'ticker'}, inplace=True)\n",
    "df_transcripts['ticker'] = df_transcripts['ticker'].astype(str).str.upper().str.strip()\n",
    "\n",
    "def get_stock_data(ticker, start, end, folder=\"stock_data\"):\n",
    "    \"\"\"\n",
    "    Load stock CSV, ensure datetime index and numeric Close, filter by date range.\n",
    "    \"\"\"\n",
    "    filename = f\"{folder}/{ticker}.csv\"\n",
    "    \n",
    "    try:\n",
    "        stock = pd.read_csv(filename, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Ensure datetime index\n",
    "    stock.index = pd.to_datetime(stock.index, errors='coerce')\n",
    "    stock = stock[stock.index.notna()]  # drop rows with bad dates\n",
    "    \n",
    "    # Ensure numeric 'Close'\n",
    "    if 'Close' not in stock.columns:\n",
    "        print(f\"No 'Close' column in {ticker}\")\n",
    "        return pd.DataFrame()\n",
    "    stock['Close'] = pd.to_numeric(stock['Close'], errors='coerce')\n",
    "    stock = stock.dropna(subset=['Close'])\n",
    "    \n",
    "    # Filter by requested date range\n",
    "    stock = stock[(stock.index >= start) & (stock.index <= end)]\n",
    "    \n",
    "    return stock\n",
    "\n",
    "def event_study_single(ticker, event_date, benchmark_file=\"stock_data/GSPC.csv\",\n",
    "                       estimation_window=60, event_window=3):\n",
    "    \"\"\"\n",
    "    Compute cumulative abnormal return (CAR) for a single event.\n",
    "    \n",
    "    ticker: stock ticker string\n",
    "    event_date: pd.Timestamp\n",
    "    benchmark_file: CSV file of benchmark (market) prices\n",
    "    estimation_window: days before event for estimating beta\n",
    "    event_window: days after/before event to calculate CAR\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define start/end of the event + estimation window\n",
    "    start = event_date - pd.Timedelta(days=estimation_window + 10)\n",
    "    end = event_date + pd.Timedelta(days=event_window + 10)\n",
    "    \n",
    "    # Load stock data\n",
    "    stock = get_stock_data(ticker, start, end)\n",
    "    if stock.empty:\n",
    "        print(f\"No stock data for {ticker} around {event_date.date()}\")\n",
    "        return None\n",
    "    \n",
    "    # Load benchmark data\n",
    "    try:\n",
    "        market = pd.read_csv(benchmark_file, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Benchmark file not found: {benchmark_file}\")\n",
    "        return None\n",
    "\n",
    "    market.index = pd.to_datetime(market.index, errors='coerce')\n",
    "    market = market[market.index.notna()]\n",
    "    \n",
    "    if 'Close' not in market.columns:\n",
    "        print(f\"No 'Close' column in benchmark {benchmark_file}\")\n",
    "        return None\n",
    "    market['Close'] = pd.to_numeric(market['Close'], errors='coerce')\n",
    "    market = market.dropna(subset=['Close'])\n",
    "    \n",
    "    # Filter by event window\n",
    "    market = market[(market.index >= start) & (market.index <= end)]\n",
    "    if market.empty:\n",
    "        print(f\"No benchmark data for {ticker} around {event_date.date()}\")\n",
    "        return None\n",
    "    \n",
    "    # Compute returns\n",
    "    stock['Return'] = stock['Close'].pct_change()\n",
    "    market['MarketReturn'] = market['Close'].pct_change()\n",
    "    \n",
    "    df = pd.concat([stock['Return'], market['MarketReturn']], axis=1).dropna()\n",
    "    if df.empty:\n",
    "        print(f\"No overlapping return data for {ticker} and benchmark around {event_date.date()}\")\n",
    "        return None\n",
    "    \n",
    "    # Estimate beta using estimation window\n",
    "    est_df = df[df.index < event_date].tail(estimation_window)\n",
    "    if len(est_df) < 2:\n",
    "        print(f\"Not enough data to estimate beta for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    beta = np.cov(est_df['Return'], est_df['MarketReturn'])[0,1] / np.var(est_df['MarketReturn'])\n",
    "    \n",
    "    # Compute abnormal returns for event window\n",
    "    event_df = df[(df.index >= event_date - pd.Timedelta(days=event_window)) &\n",
    "                  (df.index <= event_date + pd.Timedelta(days=event_window))]\n",
    "    if event_df.empty:\n",
    "        print(f\"No data in event window for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    event_df['AbnormalReturn'] = event_df['Return'] - beta * event_df['MarketReturn']\n",
    "    \n",
    "    # Cumulative abnormal return\n",
    "    CAR = event_df['AbnormalReturn'].sum()\n",
    "    \n",
    "    return CAR\n",
    "results = []\n",
    "for idx, row in df_transcripts.iterrows():\n",
    "    ticker = row['ticker']\n",
    "    event_date = row['mostImportantDateUTC']\n",
    "    CAR = event_study_single(ticker, event_date, event_window=3)\n",
    "    results.append({\"ticker\": ticker, \"event_date\": event_date, \"CAR\": CAR})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b8f6a2-88f4-452b-8bc8-32961ec80abd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## EVENT STUDY - CLEAN DATA\n",
    "# Remove duplicate rows (keep the first occurrence)\n",
    "df_car_clean = df_car.drop_duplicates()\n",
    "\n",
    "# Drop rows where CAR is NaN (or other critical fields are missing)\n",
    "df_car_clean = df_car_clean.dropna(subset=['CAR'])\n",
    "\n",
    "# Optional: drop rows where ticker or event_date is missing too\n",
    "df_car_clean = df_car_clean.dropna(subset=['ticker', 'event_date'])\n",
    "\n",
    "# Reset index for a clean DataFrame\n",
    "df_car_clean = df_car_clean.reset_index(drop=True)\n",
    "\n",
    "# Save cleaned results\n",
    "df_car_clean.to_csv(\"df_car_clean.csv\", index=False)\n",
    "\n",
    "print(df_car_clean.head(10))\n",
    "print(df_car_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161fb7b-9735-4851-9866-67df8d548dcc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## EVENT STUDY - CORRELATION WITH SENTIMENT DATA\n",
    "\n",
    "# === 1. Load Data ===\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Replace with your actual file paths\n",
    "df_car = pd.read_csv(\"df_car_clean.csv\")       # columns: ticker, event_date, CAR\n",
    "df_sent = pd.read_csv(\"df_finalized.csv\")    # columns: ticker, event_date, sentiment_score\n",
    "\n",
    "# Ensure event_date is datetime\n",
    "df_car['event_date'] = pd.to_datetime(df_car['event_date'])\n",
    "df_sent['mostImportantDateUTC'] = pd.to_datetime(df_sent['mostImportantDateUTC'])\n",
    "\n",
    "# Rename sentiment date column to match CAR dataset\n",
    "df_sent = df_sent.rename(columns={\"mostImportantDateUTC\": \"event_date\"})\n",
    "\n",
    "gvkey_ticker = pd.read_csv('merged_wrds_gvkey_V2.txt', sep='\\t')\n",
    "df_sent = df_sent.merge(gvkey_ticker[['gvkey', 'tic']], on='gvkey', how='left')\n",
    "df_sent.rename(columns={'tic': 'ticker'}, inplace=True)\n",
    "df_sent['ticker'] = df_sent['ticker'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# === 2. Merge on ticker & event_date ===\n",
    "df_merged = pd.merge(\n",
    "    df_car,\n",
    "    df_sent[['ticker', 'event_date', 'total_sentiment_score', 'average_sentiment_score']],\n",
    "    left_on=['ticker', 'event_date'],\n",
    "    right_on=['ticker', 'event_date'],\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"Merged dataset shape: {df_merged.shape}\")\n",
    "print(df_merged.head())\n",
    "\n",
    "# === 3. Clean Data ===\n",
    "df_merged = df_merged.drop(columns=['event_date'])\n",
    "\n",
    "# === 4. Correlation ===\n",
    "print(df_merged[['CAR', 'total_sentiment_score', 'average_sentiment_score']].corr())\n",
    "model = smf.ols(\"CAR ~ total_sentiment_score + average_sentiment_score\", data=df_merged).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# Scatterplot\n",
    "plt.scatter(df_merged['total_sentiment_score'], df_merged['CAR'], alpha=0.5, label=\"Total Sentiment\")\n",
    "plt.scatter(df_merged['average_sentiment_score'], df_merged['CAR'], alpha=0.5, label=\"Average Sentiment\")\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"CAR\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed9652-eb02-4cd3-96a7-17c145ab8b0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## EVENT STUDY - CORRELATION WITH SENTIMENT DATA\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Create a binary CAR column\n",
    "df_merged[\"CAR_binary\"] = (df_merged[\"CAR\"] > 0).astype(int)\n",
    "\n",
    "# Quick check\n",
    "df_merged[[\"CAR\", \"CAR_binary\"]].head()\n",
    "\n",
    "\n",
    "# Define features and target\n",
    "features = [\"total_sentiment_score\", \"average_sentiment_score\"]\n",
    "X = df_merged[features]\n",
    "X = sm.add_constant(X)  # adds intercept\n",
    "y = df_merged[\"CAR_binary\"]  # should already exist (CAR > 0 â†’ 1, else 0)\n",
    "\n",
    "# Fit the logit model\n",
    "logit_both = sm.Logit(y, X).fit()\n",
    "print(logit_both.summary())\n",
    "\n",
    "def evaluate_logit(model, df, features):\n",
    "    X = sm.add_constant(df[features])\n",
    "    y = df[\"CAR_binary\"]\n",
    "    \n",
    "    # Predictions (probabilities)\n",
    "    y_pred_prob = model.predict(X)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    auc = roc_auc_score(y, y_pred_prob)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    print(\"\\n--- Logistic Regression Evaluation ---\")\n",
    "    print(f\"Features: {features}\")\n",
    "    print(f\"AUC: {auc:.3f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "evaluate_logit(logit_both, df_merged, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7fc02-ee20-4ab0-9c57-c7636eacf3e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## EVENT STUDY FIGURES\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scatterplots with regression line\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.regplot(x=\"total_sentiment_score\", y=\"CAR\", data=df_merged, \n",
    "            scatter_kws={'alpha':0.2}, line_kws={'color':'red'})\n",
    "plt.title(\"CAR vs Total Sentiment Score\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.regplot(x=\"average_sentiment_score\", y=\"CAR\", data=df_merged, \n",
    "            scatter_kws={'alpha':0.2}, line_kws={'color':'red'})\n",
    "plt.title(\"CAR vs Average Sentiment Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"scatterplots_sentiment_vs_CAR.png\", dpi=300)  # save\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Boxplot: CAR grouped by sentiment sign\n",
    "df_merged[\"sentiment_sign\"] = (df_merged[\"average_sentiment_score\"] > 0).astype(int)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.boxplot(x=\"sentiment_sign\", y=\"CAR\", data=df_merged)\n",
    "plt.xticks([0,1], [\"Negative Sentiment\", \"Positive Sentiment\"])\n",
    "plt.title(\"CAR Distribution by Sentiment Sign\")\n",
    "plt.savefig(\"boxplot_CAR_by_sentiment.png\", dpi=300)  # save\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Histogram: CAR distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df_merged[\"CAR\"], bins=50, kde=True)\n",
    "plt.title(\"Distribution of CAR\")\n",
    "plt.xlabel(\"CAR\")\n",
    "plt.savefig(\"histogram_CAR.png\", dpi=300)  # save\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
