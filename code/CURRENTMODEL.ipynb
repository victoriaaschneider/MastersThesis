{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca15d1-d186-4fdf-bfd7-7e303e04048c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## SET UP AND IMPORTS \n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import finqual as fq\n",
    "import spacy\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import glob\n",
    "import wrds\n",
    "from yahoofinancials import YahooFinancials\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import timedelta\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5809ed5-d820-4025-a21a-e662b9ad262c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## LOAD DATA \n",
    "\n",
    "df = pd.read_csv('earnings_calls_speaker_2018_current.csv') #transcript data\n",
    "# Drop duplicate ticker column if both are identical\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "# Make sure column names are clean\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Convert mostImportantDateUTC to datetime\n",
    "df['mostImportantDateUTC'] = pd.to_datetime(df['mostImportantDateUTC'])\n",
    "\n",
    "\n",
    "# Load the GVKEY to ticker crosswalk\n",
    "gvkey_ticker = pd.read_csv('merged_wrds_gvkey_V2.txt', sep='\\t')\n",
    "\n",
    "# Load the dictionary CSV\n",
    "lm_dict = pd.read_csv('Loughran-McDonald_MasterDictionary_1993-2024.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec2bd1-2180-4c42-84fc-36e0847585ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## DATA PREPARATION\n",
    "\n",
    "# === Merge ticker info and clean ===\n",
    "# Merge gvkey ‚Üí ticker\n",
    "df = df.merge(gvkey_ticker[['gvkey', 'tic']], on='gvkey', how='left')\n",
    "\n",
    "# Rename 'tic' to 'ticker'\n",
    "df.rename(columns={'tic': 'ticker'}, inplace=True)\n",
    "\n",
    "# Remove any duplicate columns after merge\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "# Standardize ticker: strip spaces, convert to uppercase\n",
    "df['ticker'] = df['ticker'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Convert earnings call date to datetime (naive, no timezone)\n",
    "df['mostImportantDateUTC'] = pd.to_datetime(df['mostImportantDateUTC']).dt.tz_localize(None)\n",
    "\n",
    "# Optional: remove invalid tickers (like empty strings or those starting with '$')\n",
    "df = df[df['ticker'].notna() & ~df['ticker'].str.startswith('$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef057c-9a52-4133-81b1-1fe44997da71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## DATA PREPROCESSING\n",
    "\n",
    "# Load English spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 1. Load a sample of the CSV\n",
    "sample_df = pd.read_csv('earnings_calls_speaker_2018_current.csv', nrows=10000)\n",
    "\n",
    "# 2. Filter for rows where transcriptComponentTypeId == 2.0\n",
    "filtered_df = sample_df[sample_df['transcriptComponentTypeId'] == 4.0].copy()\n",
    "\n",
    "# 3. Define the cleaning function ‚Äî preserve important negations\n",
    "def split_and_clean_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    cleaned_sentences = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        tokens = []\n",
    "        for token in sent:\n",
    "            if token.is_space:\n",
    "                continue\n",
    "            # Keep negations and relevant words; remove stopwords except 'not', 'no', 'never'\n",
    "            if token.is_alpha or token.text in {\"n't\", \"not\", \"no\", \"never\"}:\n",
    "                if not token.is_stop or token.text in {\"n't\", \"not\", \"no\", \"never\"}:\n",
    "                    tokens.append(token.lemma_.lower())\n",
    "        if tokens:\n",
    "            cleaned_sentences.append(\" \".join(tokens))\n",
    "\n",
    "    return cleaned_sentences\n",
    "\n",
    "# 4. Apply the function to your column\n",
    "filtered_df['sentences'] = filtered_df['componentText'].astype(str).apply(split_and_clean_sentences)\n",
    "\n",
    "# 5. Inspect results\n",
    "print(filtered_df[['componentText', 'sentences']].head())\n",
    "\n",
    "# Map transcript date to fiscal quarter\n",
    "def get_fiscal_quarter(date):\n",
    "    month = date.month\n",
    "    year = date.year\n",
    "    if month in [1, 2, 3]:\n",
    "        return year, 1\n",
    "    elif month in [4, 5, 6]:\n",
    "        return year, 2\n",
    "    elif month in [7, 8, 9]:\n",
    "        return year, 3\n",
    "    else:\n",
    "        return year, 4\n",
    "\n",
    "df[['fiscal_year', 'fiscal_quarter']] = df['mostImportantDateUTC'].apply(lambda d: pd.Series(get_fiscal_quarter(d)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20dd35f-0db6-4431-8235-8632554b7508",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SENTIMENT ANALYSIS\n",
    "\n",
    "# Ensure column names are capitalized properly\n",
    "lm_dict.columns = lm_dict.columns.str.upper()\n",
    "\n",
    "# Convert words to lowercase for matching\n",
    "lm_dict['WORD'] = lm_dict['WORD'].str.lower()\n",
    "\n",
    "# Extract relevant word sets\n",
    "lm_positive = set(lm_dict[lm_dict['POSITIVE'] > 0]['WORD'])\n",
    "lm_negative = set(lm_dict[lm_dict['NEGATIVE'] > 0]['WORD'])\n",
    "lm_uncertainty = set(lm_dict[lm_dict['UNCERTAINTY'] > 0]['WORD'])\n",
    "lm_litigious = set(lm_dict[lm_dict['LITIGIOUS'] > 0]['WORD'])\n",
    "\n",
    "# define sentiment labeling function \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def label_sentiment_spacy(text):\n",
    "    doc = nlp(text.strip())\n",
    "    tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    pos_count = sum(token in lm_positive for token in tokens)\n",
    "    neg_count = sum(token in lm_negative for token in tokens)\n",
    "\n",
    "    if pos_count > neg_count:\n",
    "        return 'positive'\n",
    "    elif neg_count > pos_count:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "filtered_df['sentiment'] = filtered_df['sentences'].apply(\n",
    "    lambda sentences_list: [label_sentiment_spacy(sentence) for sentence in sentences_list]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc7cca-ed79-4eaa-a37d-d83384069735",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## FEATURE ENGINEERING WITH TF-IDF\n",
    "\n",
    "# SPLIT DATA INTO TESTING AND TRAINING SETS\n",
    "\n",
    "# Label sentiment for each sentence in the list\n",
    "filtered_df['sentiment'] = filtered_df['sentences'].apply(\n",
    "    lambda sents: [label_sentiment_spacy(sent) for sent in sents]\n",
    ")\n",
    "\n",
    "# 1. Explode both 'sentences' and 'sentiment' columns together\n",
    "filtered_df_exploded = filtered_df.explode(['sentences', 'sentiment']).reset_index(drop=True)\n",
    "\n",
    "# 2. Ensure sentences are strings and drop NaNs\n",
    "filtered_df_exploded['sentences'] = filtered_df_exploded['sentences'].astype(str)\n",
    "filtered_df_exploded = filtered_df_exploded.dropna(subset=['sentences', 'sentiment'])\n",
    "\n",
    "# 3. Split into train/test sets BEFORE TF-IDF\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = filtered_df_exploded['sentences']\n",
    "y = filtered_df_exploded['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# CONVERTING TO NUMERICAL FEATURES USING TF-IDF\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a16e40-1da1-4658-9e0f-cd16d02243e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## MODEL TRAINING \n",
    "\n",
    "# Train the model\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = svm_model.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save trained model + vectorizer\n",
    "joblib.dump(vectorizer, \"vectorizer.joblib\")\n",
    "joblib.dump(svm_model, \"svm_model.joblib\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[\"negative\", \"neutral\", \"positive\"])\n",
    "\n",
    "# Normalize by row (so percentages per true class)\n",
    "cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2%\", cmap=\"Blues\", \n",
    "            xticklabels=[\"negative\", \"neutral\", \"positive\"],\n",
    "            yticklabels=[\"negative\", \"neutral\", \"positive\"])\n",
    "\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.title(\"Confusion Matrix (Normalized %)\")\n",
    "plt.savefig(\"confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940d3f0-7af0-47a5-a401-894e3f24d202",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## APPLY MODEL TO FULL DATASET\n",
    "\n",
    "vectorizer = joblib.load(\"vectorizer.joblib\")\n",
    "svm_model = joblib.load(\"svm_model.joblib\")\n",
    "\n",
    "batch_size = 50000\n",
    "reader = pd.read_csv(\"earnings_calls_speaker_2018_current.csv\", chunksize=batch_size)\n",
    "\n",
    "for i, chunk in enumerate(reader):\n",
    "    print(f\"üîÑ Processing chunk {i+1}...\")\n",
    "    chunk = chunk[chunk['transcriptComponentTypeId'] == 4.0].copy()\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    chunk['sentences'] = chunk['componentText'].astype(str).apply(split_and_clean_sentences)\n",
    "    chunk_exploded = chunk.explode('sentences').dropna(subset=['sentences']).reset_index(drop=True)\n",
    "\n",
    "    X_chunk = vectorizer.transform(chunk_exploded['sentences'].astype(str))\n",
    "    preds = svm_model.predict(X_chunk)\n",
    "    chunk_exploded['pred_sentiment'] = preds\n",
    "\n",
    "    # Aggregate back to transcript level\n",
    "    sentiment_summary = (\n",
    "        chunk_exploded.groupby(['gvkey', 'mostImportantDateUTC'])\n",
    "        .agg(\n",
    "            n_sentences=('sentences', 'count'),\n",
    "            pos_share=('pred_sentiment', lambda x: np.mean(x == 'positive')),\n",
    "            neg_share=('pred_sentiment', lambda x: np.mean(x == 'negative')),\n",
    "            neu_share=('pred_sentiment', lambda x: np.mean(x == 'neutral')),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    sentiment_summary.to_parquet(f\"sentiment_batch_{i}.parquet\")\n",
    "\n",
    "\n",
    "## COMBINE SENTIMENT BATCHES\n",
    "\n",
    "files = glob.glob(\"sentiment_batch_*.parquet\")\n",
    "sentiment_all = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "sentiment_all.to_parquet(\"sentiment_all.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111e02e-fdce-4871-ab05-57fb43c09e66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load all batches (each exploded at sentence level)\n",
    "files = glob.glob(\"sentiment_batch_*.parquet\")\n",
    "sentences_all = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "\n",
    "print(f\"Total sentences loaded: {len(sentences_all)}\")\n",
    "print(sentences_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1924d-7665-4de6-b95f-d6d7338dc444",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## SENTIMENT METRICS\n",
    "\n",
    "# Load the transcript-level shares file\n",
    "sentences_all = pd.read_parquet(\"sentiment_all.parquet\")\n",
    "\n",
    "# === Derive extra metrics ===\n",
    "# 1. Total sentiment score = (#pos - #neg)\n",
    "sentences_all[\"total_sentiment_score\"] = (\n",
    "    sentences_all[\"n_sentences\"] * (sentences_all[\"pos_share\"] - sentences_all[\"neg_share\"])\n",
    ")\n",
    "\n",
    "# 2. Average sentiment score per sentence\n",
    "sentences_all[\"average_sentiment_score\"] = (\n",
    "    sentences_all[\"pos_share\"] - sentences_all[\"neg_share\"]\n",
    ")\n",
    "\n",
    "# 3. Sentiment ratio = (#pos - #neg) / total sentences\n",
    "sentences_all[\"sentiment_ratio\"] = sentences_all[\"average_sentiment_score\"]\n",
    "\n",
    "# 4. Approximate volatility (std dev of sentiment across sentences).\n",
    "#    Since we only have shares, we‚Äôll approximate using probabilities.\n",
    "#    Each sentence is +1, -1, or 0 ‚Üí variance = E[X^2] - (E[X])^2\n",
    "#    E[X] = pos_share - neg_share\n",
    "#    E[X^2] = pos_share*1^2 + neg_share*(-1)^2 + neu_share*0^2 = pos_share + neg_share\n",
    "sentences_all[\"volatility\"] = (\n",
    "    (sentences_all[\"pos_share\"] + sentences_all[\"neg_share\"])\n",
    "    - (sentences_all[\"average_sentiment_score\"] ** 2)\n",
    ") ** 0.5\n",
    "\n",
    "# Save enriched metrics\n",
    "sentences_all.to_parquet(\"sentiment_metrics.parquet\")\n",
    "sentences_all.to_csv(\"sentiment_summary.csv\", index=False)\n",
    "\n",
    "print(f\"‚úÖ Saved sentiment metrics for {len(sentences_all)} transcripts\")\n",
    "print(sentences_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a3327-bbe5-4b89-bc0a-3eea53f38b87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## MODEL EVALUATION\n",
    " \n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['positive', 'negative', 'neutral'])\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['positive', 'negative', 'neutral'],\n",
    "            yticklabels=['positive', 'negative', 'neutral'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189714c8-8551-4c02-b731-52cc3d8c952d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load transcripts\n",
    "# ==============================\n",
    "df_transcripts = pd.read_csv('earnings_calls_speaker_2018_current.csv')\n",
    "\n",
    "# Keep only transcript components with type 4.0\n",
    "df_transcripts = df_transcripts[df_transcripts['transcriptComponentTypeId'] == 4.0].copy()\n",
    "\n",
    "# ==============================\n",
    "# 2Ô∏è‚É£ Map gvkey ‚Üí ticker\n",
    "# ==============================\n",
    "# Assume you have a gvkey_ticker DataFrame with columns ['gvkey', 'tic']\n",
    "df_transcripts = df_transcripts.merge(\n",
    "    gvkey_ticker[['gvkey', 'tic']], on='gvkey', how='left'\n",
    ")\n",
    "df_transcripts.rename(columns={'tic': 'ticker'}, inplace=True)\n",
    "\n",
    "# Clean ticker column\n",
    "df_transcripts['ticker'] = df_transcripts['ticker'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Convert earnings call date to datetime\n",
    "df_transcripts['mostImportantDateUTC'] = pd.to_datetime(df_transcripts['mostImportantDateUTC']).dt.tz_localize(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de71cc-05d9-4cc1-85f1-4e812d3b0915",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##PRICE MOVEMENT FUNCTION - \n",
    "##finds stock price 2 trading days before and 2 trading days after the call datA accounts for non-trading days (weekends, holidays)\n",
    "# === Helper: normalize tickers for Yahoo Finance ===\n",
    "def normalize_ticker_for_yf(ticker):\n",
    "    ticker = ticker.strip().upper()\n",
    "    if '.' in ticker and ticker.count('.') == 1:\n",
    "        parts = ticker.split('.')\n",
    "        if parts[1].isalpha():  # e.g., BRK.A -> BRK-A\n",
    "            return f\"{parts[0]}-{parts[1]}\"\n",
    "    return ticker\n",
    "\n",
    "# === Load delisted tickers ===\n",
    "delisted_file = \"delisted_tickers.csv\"\n",
    "if os.path.exists(delisted_file):\n",
    "    delisted = set(pd.read_csv(delisted_file)['ticker'].str.upper())\n",
    "else:\n",
    "    delisted = set()\n",
    "\n",
    "# === Assume df is already loaded and cleaned with 'ticker' and 'mostImportantDateUTC' ===\n",
    "\n",
    "# === Extract unique ticker-date pairs ===\n",
    "unique_calls = df[['ticker','mostImportantDateUTC']].drop_duplicates().reset_index(drop=True)\n",
    "unique_calls['Price_Before'] = np.nan\n",
    "unique_calls['Price_After'] = np.nan\n",
    "unique_calls['Price_Pct_Change'] = np.nan\n",
    "\n",
    "# === Process each unique ticker individually ===\n",
    "tickers_to_process = unique_calls['ticker'].unique().tolist()\n",
    "print(f\"‚è≥ Processing {len(tickers_to_process)} unique tickers...\")\n",
    "\n",
    "for ticker in tickers_to_process:\n",
    "    yf_ticker = normalize_ticker_for_yf(ticker)\n",
    "    ticker_df = unique_calls[unique_calls['ticker'] == ticker]\n",
    "    start_date = ticker_df['mostImportantDateUTC'].min() - timedelta(days=10)\n",
    "    end_date = ticker_df['mostImportantDateUTC'].max() + timedelta(days=10)\n",
    "\n",
    "    try:\n",
    "        hist = yf.download(\n",
    "            yf_ticker,\n",
    "            start=start_date.strftime('%Y-%m-%d'),\n",
    "            end=end_date.strftime('%Y-%m-%d'),\n",
    "            auto_adjust=True,\n",
    "            progress=False\n",
    "        )\n",
    "        if hist.empty:\n",
    "            print(f\"‚ö†Ô∏è No data for ticker {ticker} ({yf_ticker})\")\n",
    "            delisted.add(ticker)\n",
    "            continue\n",
    "\n",
    "        hist.index = hist.index.tz_localize(None)\n",
    "        hist = hist.sort_index()\n",
    "\n",
    "        for idx, row in ticker_df.iterrows():\n",
    "            call_date = row['mostImportantDateUTC']\n",
    "            before_dates = hist.index[hist.index < call_date]\n",
    "            after_dates = hist.index[hist.index > call_date]\n",
    "\n",
    "            if len(before_dates) >= 2 and len(after_dates) >= 2:\n",
    "                day_before = before_dates[-2]\n",
    "                day_after = after_dates[1]\n",
    "\n",
    "                try:\n",
    "                    # ‚ö° Fix FutureWarning by using .iloc[0]\n",
    "                    price_before = float(hist.loc[day_before]['Close'].iloc[0])\n",
    "                    price_after = float(hist.loc[day_after]['Close'].iloc[0])\n",
    "\n",
    "                    unique_calls.at[idx, 'Price_Before'] = price_before\n",
    "                    unique_calls.at[idx, 'Price_After'] = price_after\n",
    "                    unique_calls.at[idx, 'Price_Pct_Change'] = (price_after - price_before) / price_before\n",
    "\n",
    "                except Exception as e:\n",
    "                    unique_calls.at[idx, 'Price_Before'] = np.nan\n",
    "                    unique_calls.at[idx, 'Price_After'] = np.nan\n",
    "                    unique_calls.at[idx, 'Price_Pct_Change'] = np.nan\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to download {ticker} ({yf_ticker}): {e}\")\n",
    "        delisted.add(ticker)\n",
    "\n",
    "# === Save unique price movements to CSV ===\n",
    "unique_calls.to_csv('price_movement.csv', index=False)\n",
    "print(f\"‚úÖ Saved {len(unique_calls)} rows to price_movement.csv\")\n",
    "\n",
    "# === Merge back to full dataset ===\n",
    "df = df.merge(\n",
    "    unique_calls,\n",
    "    on=['ticker','mostImportantDateUTC'],\n",
    "    how='left',\n",
    "    suffixes=('','_y')\n",
    ")\n",
    "\n",
    "df['Price_Before'] = df['Price_Before_y']\n",
    "df['Price_After'] = df['Price_After_y']\n",
    "df['Price_Pct_Change'] = df['Price_Pct_Change_y']\n",
    "df.drop(columns=['Price_Before_y','Price_After_y','Price_Pct_Change_y'], inplace=True)\n",
    "\n",
    "# === Save updated delisted tickers ===\n",
    "safe_delisted = [t for t in delisted if isinstance(t, str)]\n",
    "pd.Series(sorted(safe_delisted), name=\"ticker\").to_csv(delisted_file, index=False)\n",
    "print(f\"‚úÖ Updated delisted tickers saved to {delisted_file}\")\n",
    "\n",
    "print(\"‚úÖ Price movement merge complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6c435-9640-466e-a207-d7b4da421100",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## PULL FINANCIAL METRICS USING WRDS - PULLING REVENUE, EPS, P/E RATIO, AND EV/EBITDA\n",
    "\n",
    "# ===== Connect to WRDS =====\n",
    "conn = wrds.Connection(wrds_username='YOUR_USERNAME')  # replace with your WRDS username\n",
    "\n",
    "# ===== Pull quarterly financials from Compustat =====\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    gvkey,\n",
    "    tic AS ticker,\n",
    "    datadate,\n",
    "    revtq AS revenue,         -- quarterly revenue\n",
    "    epspxq AS eps,            -- quarterly EPS\n",
    "    prccq/NULLIF(epspxq,0) AS pe_ratio, -- quarterly P/E\n",
    "    (prccq * cshoq)/oiadpq AS ev_ebitda  -- quarterly EV/EBITDA\n",
    "FROM comp.fundq\n",
    "WHERE datadate >= '2018-01-01' \n",
    "  AND datadate <= '2022-12-31'\n",
    "  AND oiadpq IS NOT NULL\n",
    "  AND oiadpq != 0\n",
    "  AND epspxq IS NOT NULL\n",
    "  AND epspxq != 0\n",
    "\"\"\"\n",
    "\n",
    "df_financials = conn.raw_sql(query)\n",
    "\n",
    "# ===== Clean tickers and dates =====\n",
    "df_financials['ticker'] = df_financials['ticker'].str.upper().str.strip()\n",
    "df_financials['datadate'] = pd.to_datetime(df_financials['datadate'])\n",
    "\n",
    "# ===== Optional: remove invalid tickers =====\n",
    "df_financials = df_financials[df_financials['ticker'].notna() & ~df_financials['ticker'].str.startswith('$')]\n",
    "\n",
    "# ===== Sort by ticker & date =====\n",
    "df_financials = df_financials.sort_values(['ticker', 'datadate']).reset_index(drop=True)\n",
    "\n",
    "print(df_financials.head())\n",
    "print(df_financials.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aec025-0d81-4978-bc5f-874030a78e21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## FINANCIAL DICTIONARY OF ALL TICKERS\n",
    "\n",
    "# Ensure tickers are strings\n",
    "df_transcripts['ticker'] = df_transcripts['ticker'].astype(str)\n",
    "df_financials['ticker'] = df_financials['ticker'].astype(str)\n",
    "\n",
    "# Strip time from datetime\n",
    "df_transcripts['mostImportantDateUTC'] = pd.to_datetime(df_transcripts['mostImportantDateUTC']).dt.floor('D')\n",
    "df_financials['datadate'] = pd.to_datetime(df_financials['datadate']).dt.floor('D')\n",
    "\n",
    "# Filter to common tickers\n",
    "common_tickers = set(df_transcripts['ticker']).intersection(df_financials['ticker'])\n",
    "df_transcripts = df_transcripts[df_transcripts['ticker'].isin(common_tickers)].copy()\n",
    "df_financials = df_financials[df_financials['ticker'].isin(common_tickers)].copy()\n",
    "\n",
    "# Sort financials by ticker and date\n",
    "df_financials = df_financials.sort_values(['ticker', 'datadate'])\n",
    "\n",
    "# Build a dictionary of financials per ticker for fast lookup\n",
    "financials_dict = {ticker: g[['datadate', 'revenue', 'eps', 'pe_ratio', 'ev_ebitda']].values\n",
    "                   for ticker, g in df_financials.groupby('ticker')}\n",
    "\n",
    "# Function to get next financial row after transcript\n",
    "def get_next_financial(row):\n",
    "    ticker = row['ticker']\n",
    "    date = row['mostImportantDateUTC']\n",
    "    if ticker not in financials_dict:\n",
    "        return pd.Series([None]*5, index=['datadate', 'revenue', 'eps', 'pe_ratio', 'ev_ebitda'])\n",
    "    \n",
    "    # All financial dates for this ticker\n",
    "    arr = financials_dict[ticker]\n",
    "    for datadate, revenue, eps, pe, ev in arr:\n",
    "        if datadate >= date:\n",
    "            return pd.Series([datadate, revenue, eps, pe, ev], index=['datadate', 'revenue', 'eps', 'pe_ratio', 'ev_ebitda'])\n",
    "    return pd.Series([None]*5, index=['datadate', 'revenue', 'eps', 'pe_ratio', 'ev_ebitda'])\n",
    "\n",
    "# Apply to all transcripts\n",
    "df_merged = df_transcripts.copy()\n",
    "df_merged[['datadate', 'revenue', 'eps', 'pe_ratio', 'ev_ebitda']] = df_merged.apply(get_next_financial, axis=1)\n",
    "\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fe7f0-c502-4276-a86b-d9ef28e87baf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## MERGE FINANCIALS AND PRICE MOVEMENT DATASETS\n",
    "\n",
    "# ==========================\n",
    "# 1. Load price movement data\n",
    "# ==========================\n",
    "df_price = pd.read_csv(\"price_movement.csv\")\n",
    "\n",
    "# Convert datetime column\n",
    "df_price['mostImportantDateUTC'] = pd.to_datetime(df_price['mostImportantDateUTC'])\n",
    "\n",
    "# Ensure tickers are strings\n",
    "df_price['ticker'] = df_price['ticker'].astype(str)\n",
    "\n",
    "# ==========================\n",
    "# 2. Prepare merged transcript + financials dataset\n",
    "# ==========================\n",
    "# Ensure datetime and ticker types match\n",
    "df_merged['mostImportantDateUTC'] = pd.to_datetime(df_merged['mostImportantDateUTC'])\n",
    "df_merged['mostImportantDateUTC'] = pd.to_datetime(df_merged['mostImportantDateUTC']).dt.floor('D')\n",
    "df_price['mostImportantDateUTC'] = pd.to_datetime(df_price['mostImportantDateUTC']).dt.floor('D')\n",
    "df_merged['ticker'] = df_merged['ticker'].astype(str)\n",
    "\n",
    "# ==========================\n",
    "# 3. Merge datasets\n",
    "# ==========================\n",
    "df_analysis = pd.merge(\n",
    "    df_merged,\n",
    "    df_price,\n",
    "    on=['ticker', 'mostImportantDateUTC'],\n",
    "    how='left'   # keep only matches\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 4. Check the merged result\n",
    "# ==========================\n",
    "print(df_analysis.head())\n",
    "print(f\"Merged dataset shape: {df_analysis.shape}\")\n",
    "print(df_analysis.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b457367-46c4-4cf3-a8f6-f4b75e721768",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## MERGE FINANCIALS AND PRICE MOVEMENT WITH SENTIMENT METRICS FOR ANALYSIS\n",
    "\n",
    "# ==========================\n",
    "# 1. Ensure datetime and ID alignment\n",
    "# ==========================\n",
    "df_analysis['mostImportantDateUTC'] = pd.to_datetime(df_analysis['mostImportantDateUTC']).dt.floor('D')\n",
    "df_summary['mostImportantDateUTC'] = pd.to_datetime(df_summary['mostImportantDateUTC']).dt.floor('D')\n",
    "\n",
    "# Ensure tickers are strings (or use gvkey if preferred)\n",
    "df_analysis['gvkey'] = df_analysis['gvkey'].astype(str)\n",
    "df_summary['gvkey'] = df_summary['gvkey'].astype(str)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 2. Merge sentiment metrics\n",
    "# ==========================\n",
    "# Pick the columns you want to add\n",
    "sentiment_cols = ['gvkey', 'mostImportantDateUTC', \n",
    "                  'total_sentiment_score', 'average_sentiment_score', \n",
    "                  'sentiment_ratio', 'volatility']\n",
    "\n",
    "df_final = pd.merge(\n",
    "    df_analysis,\n",
    "    df_summary[sentiment_cols],\n",
    "    on=['gvkey', 'mostImportantDateUTC'],\n",
    "    how='left'  # keep all rows from df_analysis\n",
    ")\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 3. Check the merged dataset\n",
    "# ==========================\n",
    "\n",
    "\n",
    "\n",
    "print(df_final.head())\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(df_final.isna().sum())\n",
    "\n",
    "df_final.to_csv('df_finalized_updated.csv', index=False)\n",
    "df_final.to_parquet('df_finalized_updated.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995a2a0-59e9-488b-a9f5-39173390cf99",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## AGGREGATE DATA TO TRANSCRIPT LEVEL\n",
    "\n",
    "df_transcript_level = df_final.groupby(['gvkey', 'mostImportantDateUTC']).agg({\n",
    "    'total_sentiment_score':'first',\n",
    "    'average_sentiment_score':'first',\n",
    "    'sentiment_ratio':'first',\n",
    "    'volatility':'first',\n",
    "    'Price_Pct_Change':'first',\n",
    "    'revenue':'first',\n",
    "    'eps':'first',\n",
    "    'pe_ratio':'first',\n",
    "    'ev_ebitda':'first',\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "print(df_transcript_level.head())\n",
    "print(f\"Final dataset shape: {df_transcript_level.shape}\")\n",
    "print(df_transcript_level.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20422a-e6e8-4c1f-84ac-1023554119c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## CALCULATING CORRELATIONS\n",
    "\n",
    "# ===== 5. Correlation with price change =====\n",
    "corr = df_finalized[sentiment_cols + ['Price_Pct_Change']].corr()\n",
    "print(\"Correlation with Price_Pct_Change:\\n\", corr['Price_Pct_Change'].sort_values(ascending=False))\n",
    "\n",
    "# Scatter plots\n",
    "for col in sentiment_cols:\n",
    "    sns.scatterplot(data=df_final, x=col, y='Price_Pct_Change')\n",
    "    plt.title(f'{col} vs Price_Pct_Change')\n",
    "    plt.show()\n",
    "\n",
    "# ===== 6. Correlation with financial metrics =====\n",
    "financial_cols = ['revenue', 'eps', 'pe_ratio', 'ev_ebitda']\n",
    "\n",
    "for fin_col in financial_cols:\n",
    "    print(f'\\n--- {fin_col} ---')\n",
    "    corr = df_final[sentiment_cols + [fin_col]].corr()\n",
    "    print(corr[fin_col].sort_values(ascending=False))\n",
    "\n",
    "# ===== 7. Simple OLS regression: sentiment -> price change =====\n",
    "X = df_final[sentiment_cols]\n",
    "y = df_final['Price_Pct_Change']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X, missing='drop').fit()\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
